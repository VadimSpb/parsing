# Методы сбора и обработки данных из сети Интернет
> **Geek University Data Engineering**

_В данном курсе были выполнены следующие задания._


## Урок 1. Основы клиент-серверного взаимодействия. Парсинг API
1. Посмотреть документацию к API GitHub, 
разобраться как вывести список репозиториев для конкретного пользователя, 
сохранить JSON-вывод в файле *.json. <br>
[Решение](https://github.com/bostspb/parsing/blob/master/lesson01/task01.py)
 
2. Изучить список открытых API (https://www.programmableweb.com/category/all/apis). 
Найти среди них любое, требующее авторизацию (любого типа). 
Выполнить запросы к нему, пройдя авторизацию. 
Ответ сервера записать в файл.<br>
Если нет желания заморачиваться с поиском, 
возьмите API вконтакте (https://vk.com/dev/first_guide). 
Сделайте запрос, чтобы получить список всех сообществ на которые вы подписаны.<br>
[Решение](https://github.com/bostspb/parsing/blob/master/lesson01/task02.py)


## Урок 2. Парсинг HTML. BeautifulSoup, MongoDB
Необходимо собрать информацию о вакансиях на вводимую должность 
(используем input или через аргументы) с сайтов Superjob и HH. 
Приложение должно анализировать несколько страниц сайта 
(также вводим через input или аргументы). 
Получившийся список должен содержать в себе минимум:
- Наименование вакансии.
- Предлагаемую зарплату (отдельно минимальную, максимальную и валюту).
- Ссылку на саму вакансию.
- Сайт, откуда собрана вакансия.

По желанию можно добавить ещё параметры вакансии 
(например, работодателя и расположение). 
Структура должна быть одинаковая для вакансий с обоих сайтов. 
Общий результат можно вывести с помощью dataFrame через pandas.

[Решение](https://github.com/bostspb/parsing/blob/master/lesson02/task01.py)
<br>
>_Для задания был выбран собственный рабочий проект, в котором требовалось
собрать данные со справочника банков - bankchart.ru. 
Результат сбора сохранял в файл в формате JSON._


## Урок 3. Системы управления базами данных MongoDB и SQLite в Python
1. Развернуть у себя на компьютере/виртуальной машине/хостинге MongoDB 
и реализовать функцию, записывающую собранные вакансии в созданную БД.
<br> [Решение](https://github.com/bostspb/parsing/blob/master/lesson03/task01.py)
(_Вместо вакансий взял информацию о банках из прошлого урока_)

2. Написать функцию, которая производит поиск и выводит на экран вакансии 
с заработной платой больше введённой суммы. 
Запрос должен анализировать одновременно минимальную и максимальную зарплату.
<br> [Решение](https://github.com/bostspb/parsing/blob/master/lesson03/task02.py)
(_Реализовал поиск банков, по которым недособирали расширенную информацию_)

3. Написать функцию, которая будет добавлять в вашу базу данных 
только новые вакансии с сайта.
<br> [Решение](https://github.com/bostspb/parsing/blob/master/lesson03/task03.py)
(_Реализовал добавление информации только по новым банкам_)


## Урок 4. Парсинг HTML. XPath
1. Написать приложение, которое собирает основные новости с сайтов 
news.mail.ru, lenta.ru, yandex-новости. 
Для парсинга использовать XPath. 
Структура данных должна содержать:<br>
[] _название источника;_<br>
[] _наименование новости;_<br>
[] _ссылку на новость;_<br>
[] _дата публикации._

2. Сложить собранные данные в БД

[Решение](https://github.com/bostspb/parsing/tree/master/lesson04)
<br>
_Парсер под каждый сайт был реализован в виде отдельного класса: 
[LentaRuParser](https://github.com/bostspb/parsing/blob/master/lesson04/lenta_ru.py),
[MailRuParser](https://github.com/bostspb/parsing/blob/master/lesson04/mail_ru.py),
[YandexRuParser](https://github.com/bostspb/parsing/blob/master/lesson04/yandex_ru.py), 
а запуск парсинга и сохранение в базу MongoDB вынесен в отдельный скрипт 
[main.py](https://github.com/bostspb/parsing/blob/master/lesson04/main.py)_


## Урок 5. Selenium в Python
Написать программу, которая собирает входящие письма из своего 
или тестового почтового ящика и сложить данные о письмах в базу данных 
(от кого, дата отправки, тема письма, текст письма полный)
<br>[Решение](https://github.com/bostspb/parsing/blob/master/lesson05/main.py)


## Урок 6. Scrapy
**I вариант**
1. Доработать паука в имеющемся проекте, чтобы он формировал item по структуре:
<br>[] _Наименование вакансии_
<br>[] _Зарплата от_
<br>[] _Зарплата до_
<br>[] _Ссылку на саму вакансию_
<br>[] _Сайт откуда собрана вакансия_
<br>И складывал все записи в БД (любую)

2. Создать в имеющемся проекте второго паука по сбору вакансий с сайта _superjob_. 
Паук должен формировать item'ы по аналогичной структуре и складывать 
данные также в БД

**II вариант**
1) Создать двух пауков по сбору данных о книгах с сайтов labirint.ru и book24.ru
2) Каждый паук должен собирать:
<br>[] _Ссылку на книгу_
<br>[] _Наименование книги_
<br>[] _Автор(ы)_
<br>[] _Основную цену_
<br>[] _Цену со скидкой_
<br>[] _Рейтинг книги_

3) Собранная информация должна складываться в базу данных

**Решение**: [Pull Request](https://github.com/bostspb/parsing/pull/6/files),
[каталог с проектом Scrapy](https://github.com/bostspb/parsing/tree/master/lesson06)

>_Для задания, также как и в Уроке 2, был выбран собственный рабочий проект, 
в котором требовалось собрать данные со справочника банков - bankchart.ru._


## Урок 7. 
1. Взять любую категорию товаров на сайте Леруа Мерлен. 
Собрать с использованием `ItemLoader` следующие данные:
<br>[] _название;_
<br>[] _все фото;_
<br>[] _параметры товара в объявлении;_
<br>[] _ссылка;_
<br>[] _цена._

2. С использованием `output_processor` и `input_processor` реализовать 
очистку и преобразование данных. Цены должны быть в виде числового значения.

3. Написать универсальный обработчик параметров объявлений, 
который будет формировать данные вне зависимости от их типа и количества.

4. Реализовать хранение скачиваемых файлов в отдельных папках, 
каждая из которых должна соответствовать собираемому товару.

**Решение**: [Pull Request](https://github.com/bostspb/parsing/pull/7/files),
[каталог с проектом Scrapy](https://github.com/bostspb/parsing/tree/master/lesson07)
